# encoding = UTF-8

import argparse
import pickle as pickle
import pandas as pd
import os
import sys
import pyfaidx
import math
import re

parser = argparse.ArgumentParser(description='two-group DE')
parser.add_argument('--ref', type=str, required=True)
parser.add_argument('--countmatrix', type=str, required=True)
parser.add_argument('--n_genes', type=int, required=False, default=7)
parser.add_argument('--quant', type=int, required=False, default=5)
parser.add_argument('--fold_changes', type=int, required=False)
parser.add_argument('--output', type=str, required=False)

args = parser.parse_args()
ref = args.ref
table = args.countmatrix
n_levels = args.quant
genes = args.n_genes
modelName = args.output


def readTable(file):
	
	df_table = pd.read_table(file)
	# print(df_table.info())
	# remove all rows where expected counts is less than 1
	df1 = df_table[df_table.expected_count >= 1]
	df1.drop(df1.index)
	# sort the dataframe in ascending order by the expected count column
	df_sorted = df1.sort_values(by=['expected_count'], ascending=True)
	
	# Set the classes of expression level
	levels = ['very_low', 'low', 'normal', 'high', 'very_high']
	# divide the dataframe into quantiles and randomly sample the number of specified genes from each class
	df_genes = df_sorted.groupby(pd.qcut(df_sorted.expected_count, n_levels, labels=levels)).apply(
		lambda x: x.sample(genes))
	# Subtract the selected genes to be differentially expressed
	baseline = pd.concat([df_sorted, df_genes, df_genes]).drop_duplicates(keep=False)
	
	# create two groups: case and control
	df_group1 = df_genes.copy()
	df_group2 = df_genes.copy()
	
	return baseline, df_group1, df_group2


def fold_change():
	
	FC = [x * 0.5 for x in range(1, genes + 1)]
	
	return FC


def induce_fold_change(FC, value, *kwargs, ):
	for i in FC:
		for ag in kwargs:
			if ag == 'control':
				count = value * math.sqrt(i)
				return count
			elif ag == 'case':
				count = value * 1 / math.sqrt(i)
				return count


# return the transcript offsets and associated counts


def parseIndexRef(indexFile):
	"""
	Description:
	Read in sequence data from reference index FASTA file returns a list of transcript IDs
	offset, seqLen, position
	Parameters
	 - indexFile (str): The index file generated by the program, written to the current directory
	Return: The function returns a list of tuples containing the transcript id, start and end offset of the transcript sequence
	"""
	ref_inds = []
	filt_ref_inds = []
	
	try:
		fai = open(indexFile, 'r')
	except BaseException:
		print()
		
		sys.exit('Cannot find indexed reference file. Please provide a reference FASTA file')
	
	for line in fai:
		splt = line[:-1].split('\t')
		header = '@' + splt[0]
		seqLen = int(splt[1])
		offset = int(splt[2])
		lineLn = int(splt[3])
		nLines = seqLen / lineLn
		
		if seqLen % lineLn != 0:
			nLines += 1
		ref_inds.append([header, offset, offset + seqLen + nLines, seqLen])
	for i in ref_inds:
		if i[3] >= 400:
			filt_ref_inds.append(i)
	for x in filt_ref_inds:
		x.pop(3)
	fai.close()
	return filt_ref_inds


def create_model(ref, profile):
	
	df_ref = pd.DataFrame(ref, columns=['Transcript_ID', 'start', 'end', 'Length'])
	del df_ref['Length']
	# print(df_ref.info())
	df_ref['ENS_transcript_id'] = df_ref['Transcript_ID'].apply(lambda x: re.sub(r"^>(ENST\d*\.\d{1,3}(?:_.*)*)\|.*", r"\1", x))
	df_table = profile
	df_result = pd.merge(df_table, df_ref, left_on='transcript_id', right_on='ENS_transcript_id')
	df_result = df_result[['Transcript_ID', 'start', 'end', 'expected_count']]
	df_result['expected_count'] = df_result['expected_count'].round().astype(int)
	total = df_result['expected_count'].sum()
	df_result['proportional_count'] = df_result['expected_count'].div(total)

	return df_result


def main():
	
	basename = str(os.path.basename(modelName))
	os.symlink(ref, basename)
	pyfaidx.Faidx(basename)
	cwd = os.getcwd()
	indexFile = ''
	for file in os.listdir(cwd):
		if file.endswith('.fai'):
			indexFile = (os.path.join('.', file))
	
	df = readTable(table)
	baseline = df[0]
	case = df[1]
	control = df[2]
	case['expected_count'] = case['expected_count'].apply(lambda x: induce_fold_change(fold_change(), x, 'case'))
	control['expected_count'] = control['expected_count'].apply(
		lambda x: induce_fold_change(fold_change(), x, 'control'))
	
	# combine baseline and group
	group_A = pd.concat([baseline, control], ignore_index=True, sort=False)
	group_B = pd.concat([baseline, case], ignore_index=True, sort=False)
	
	refindex = parseIndexRef(indexFile)
	reference = refindex[0]
	
	model_A = create_model(reference, group_B)
	model_B = create_model(reference, group_A)
	
	group_A = model_A.to_records(index=False)
	group_B = model_B.to_records(index=False)
	records = (group_A, group_B)
	
	outf = modelName + '.p'
	output = open(outf, 'wb')
	pickle.dump(records, output)


if __name__ == '__main__':
	main()

