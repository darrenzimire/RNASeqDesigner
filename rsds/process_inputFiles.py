# encoding=utf-8

import pickle as pickle
import numpy as np
import gzip
import itertools
import random
from rsds import distributions, output
from rsds.utilities import sequence_handling
import sys
import logging.handlers

errlog = logging.getLogger("ErrLog")


def defaultfragsize(fragment_size, fragment_std, counts):

	RFS = []
	FS = np.random.normal(fragment_size, fragment_std, 100000).astype(int)
	# print(FS)
	for i in counts:
		randomFS = random.choices(FS, k=i)
		# print(randomFS)
		RFS.append(randomFS)

	return RFS


def find_nearest(array, lower_bound, upper_bound):
	array = np.asarray(array)
	idx_lower = (np.abs(array - lower_bound)).argmin()
	idx_upper = (np.abs(array - upper_bound)).argmin()
	
	return idx_lower, idx_upper


def parseIndexRef(indexFile):
	"""
	Description:
	Read in sequence data from reference index FASTA file returns a list of transcript IDs
	offset, seqLen, position
	Parameters
	 - indexFile (str): The index file generated by the program, written to the current directory
	Return: The function returns a list of tuples containing the transcript id, start and end offset of the transcript sequence
	"""
	ref_inds = []
	filt_ref_inds = []

	try:

		fai = open(indexFile, 'r')
	except BaseException:
		print()
		errlog.error('Cannot find indexed reference file. Please provide a reference FASTA file')
		sys.exit('Cannot find indexed reference file. Please provide a reference FASTA file')

	for line in fai:
		splt = line[:-1].split('\t')
		header = '>' + splt[0]
		seqLen = int(splt[1])
		offset = int(splt[2])
		lineLn = int(splt[3])
		nLines = seqLen / lineLn

		if seqLen % lineLn != 0:
			nLines += 1
		ref_inds.append([header, offset, offset + seqLen + nLines, seqLen])
	for i in ref_inds:
		if i[3] >= 400:
			filt_ref_inds.append(i)
	for x in filt_ref_inds:
		x.pop(3)
	fai.close()

	return filt_ref_inds


def get_transcriptdata(model, refindex):

	model_data = process_models(model)
	transcript_identities = model_data[0]
	counts = model_data[1]
	propcounts = model_data[2]
	transcript_sequences =[]

	for i in transcript_identities:
		p = sequence_handling.processTransIDs(refindex, [i])
		for id, seq in p.items():
			transcript_identities.append(id)
			transcript_sequences.append(seq)

	return transcript_sequences, counts, propcounts


def default_simulation(refFile, refindex, readtot):

	counts = []
	transcript_sequences = []
	# ref_transcript_ids = parseIndexRef(refindex)
	NB_counts = distributions.negative_binomial()
	counts_NB = np.random.choice(NB_counts, size=readtot, replace=True).tolist()
	scaled_counts = sequence_handling.scalereadnum(counts_NB, readtot)
	samptransids = random.choices(refindex, k=len(scaled_counts))

	for i in samptransids:
		p = sequence_handling.processTransIDs(refFile, [i])
		for id, seq in p.items():
			# transcript_identities.append(id)
			transcript_sequences.append(seq)
	# transcript_ids.append(samptransids)
	counts.append(scaled_counts)

	return transcript_sequences, counts


def samplingtranscripts(ids, readtot):
	""""
	Description: This function randomly sample from all reference transcripts
	Parameters: ids (list of tuples) It takes as input all reference transcripts offsets
	Returns: This function returns a subset of transcript ids to be sampled from
	"""
	random.seed(seed)
	numreads = readtot
	sampledtranscripts = random.sample(ids, numreads)

	return sampledtranscripts


def proc_FLmodel(file, n):

	f = gzip.open(file, 'rb')
	model = pickle.load(f)

	mus = model[0].flatten()
	sigma = np.sqrt(model[1].flatten())
	weights = model[2]
	aic = model[3]
	bic = model[4]
	size = [int(round(i * n)) for i in weights]
	np.random.seed(1234)
	sample = []
	for m, sc, si in zip(mus, sigma, size):
		N = np.random.normal(loc=m, scale=sc, size=si)

		sample.append(N.tolist())

	merged_dist = list(itertools.chain.from_iterable(sample))
	res = np.sort(np.round(np.exp(merged_dist)))
	
	return res


def sample_target(dist, readlen, seqlen, counts):

	bounds = find_nearest(dist, readlen, seqlen)
	target_sample = dist[bounds[0]:bounds[1]]
	randomFS = random.choices(target_sample, k=counts)
	
	return randomFS


def compilefastqrecord(readlen, reference, refindex, filename, kwargs, model=None, readtot=None, fragmodel=None):
	# global counts, transcript_sequences, Fraglen_dist
	counts = []
	transcript_sequences = []
	Fraglen_dist = []

	if model == None:
		default_mode = default_simulation(reference, refindex, readtot)
		trans_seq = list(default_mode[0])
		transcript_sequences.append(trans_seq)
		count_data = list(itertools.chain.from_iterable(default_mode[1]))
		counts.append(count_data)

	if model != None:
		transcript_info = get_transcriptdata(model, refindex)
		transcript_sequences = transcript_info[0]
		if readtot != None:
			counts = np.rint(np.multiply(transcript_info[2], readtot)).astype(int)
			counts.append(counts)
		else:
			counts = transcript_info[1]
			counts.append(counts)

	if fragmodel != None:
		FRS = proc_FLmodel(fragmodel, readtot).astype(int)
		Fraglen_dist.append(FRS)

	else:
		Fraglen_dist = defaultfragsize(250, 25, counts[0])
		Fraglen_dist.append(Fraglen_dist)

	if kwargs =='se':
		for seq, r in zip(transcript_sequences[0], counts[0]):
			# print(seq)
			readinfo = sequence_handling.GenerateRead(seq, readlen, r, 'SE')
			startpos = readinfo[0]
			endpos = readinfo[1]
			for index, (i, j) in enumerate(zip(startpos[0], endpos[0])):
				header = output.sequence_identifier(readlen, index)
				read = seq[int(i):int(j)]
				output.write_fastq(filename, header, read, 'se')

	elif kwargs == 'pe':

		for seq, r in zip(transcript_sequences, Fraglen_dist):
			readinfo = sequence_handling.GenerateRead(seq, readlen, r, 'PE')
			startpos = readinfo[0]
			endpos = readinfo[1]
			R1 = []
			R2 = []
			for index, (i, j) in enumerate(zip(startpos[0], endpos[0])):
				read = seq[int(i):int(j)]
				data = sequence_handling.process_reads_PE(readlen, read)
				R1.append(''.join(data[0]))
				R2.append(''.join(data[1]))
			for index, (r1, r2) in enumerate(zip(R1, R2)):
				header = output.sequence_identifier(readlen, index)
				read1 = r1
				read2 = r2
				output.write_fastq(filename, header, read1, read2)


def process_models(model):

	transcript_ID = []
	transcript_count = []
	transcript_propcount = []
	file = gzip.open(model, 'rb')
	profile = pickle.load(file)

	for i in profile:
		rec_id = i[0]
		trans_id = rec_id.split('|')
		id = (trans_id[0], i[1], i[2])
		transcript_ID.append(id)
		rec_counts = np.rint(i[3]).astype(int)
		transcript_count.append(rec_counts)
		rec_propcount = i[4]
		transcript_propcount.append(rec_propcount)

	return transcript_ID, transcript_count, transcript_propcount


def proc_DEmodel(model):
	
	file = open(model, 'rb')
	model = pickle.load(file)
	background = model[0]
	control = model[1]
	experiment = model[2]
	return background, control, experiment


def expression_profiling_simulation(model, readtot):
	
	profile = process_models(model)

	if readtot==None:
		transcript_ids = profile[0]
		counts = profile[1]
		return transcript_ids, counts

	elif readtot != None:
		counts_s = np.rint(np.multiply(profile[2], readtot)).astype(int)
		transcript_ids = profile[0]
		return transcript_ids, counts_s


def writeDEoutput(diffmodel):

	model = proc_DEmodel(diffmodel)
	background = process_models(model[0])
	control = process_models(model[1])
	experiment = process_models(model[2])


def proc_qualmodel(model):
	pass


def process_fastq(file):
	pass


def process_SAM(file):
	pass


def proc_FASTA():
	pass


	# for seq, r in zip(transcript_sequences, counts):
	# 	randomFS = sample_target(Fraglen_dist, readlen, len(seq), r)
	# 	print(randomFS)

	# if model==None:
	#
	# 	errlog.info(print('Simulating single-end reads....' + "\n"))
	# 	errlog.info(print('No transcript profile model detected!!' + "\n"))
	# 	errlog.info(print('Simulating default transcript profile' + "\n"))
	# 	default_mode = default_simulation(reference, refindex, readtot)
	# 	transcript_sequences = default_mode[0]
	# 	# print(len(transcript_sequences))
	# 	counts = list(itertools.chain.from_iterable(default_mode[1]))
	# 	# print(sum(counts))

	# else:
	# 	transcript_info = get_transcriptdata(model, refindex)
	# 	transcript_sequences = transcript_info[0]
	#
	# 	if readtot != None:
	# 		counts = np.rint(np.multiply(transcript_info[2], readtot)).astype(int)
	# 	else:
	# 		counts = transcript_info[1]

	# if fragmodel != None:
	# 	Fraglen_dist = proc_FLmodel(fragmodel, readtot).astype(int)
	# 	for seq, r in zip(transcript_sequences, counts):
	# 		randomFS = sample_target(Fraglen_dist, readlen, len(seq), r)
	# 		print(randomFS)
	#
	# else:
	# 	Fraglen_dist = defaultfragsize(250, 25, counts)
		# print(Fraglen_dist)

		# print(randomFS)
		# randomFragmentsizes.append(randomFS)
